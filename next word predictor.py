# -*- coding: utf-8 -*-
"""chatbot_test1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A58dXdOMaLvUVUHXeEiGkNmiEXTgMK76
"""

!pip install transformers datasets evaluate --quiet
!pip install -U fsspec==2023.6.0

import torch
import math
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import load_dataset

dataset = load_dataset("wikitext", "wikitext-2-raw-v1")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

def tokenize_function(examples):
    return tokenizer(examples["text"], return_special_tokens_mask=True)

tokenized = dataset.map(tokenize_function, batched=True, remove_columns=["text"])

block_size = 128
def group_texts(examples):
    joined = {k: sum(examples[k], []) for k in examples}
    total_len = len(joined["input_ids"])
    total_len = (total_len // block_size) * block_size
    return {
        k: [t[i : i + block_size] for i in range(0, total_len, block_size)]
        for k, t in joined.items()
    }

lm_datasets = tokenized.map(group_texts, batched=True)

model = GPT2LMHeadModel.from_pretrained("gpt2")
model.resize_token_embeddings(len(tokenizer))


training_args = TrainingArguments(
    output_dir="./gpt2-nextword",
    overwrite_output_dir=True,
    per_device_train_batch_size=2,
    num_train_epochs=2,
    logging_steps=100,
    save_strategy="no",
    eval_strategy="no",
    report_to="none",
    fp16=torch.cuda.is_available(),
)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_datasets["train"],
    data_collator=data_collator,
)

trainer.train()


def predict_next_word(input_text, top_k=1):
    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model(**inputs)
    next_token_logits = outputs.logits[:, -1, :]
    probs = torch.softmax(next_token_logits, dim=-1)
    top_k_probs, top_k_ids = torch.topk(probs, k=top_k, dim=-1)
    predictions = [tokenizer.decode([token_id]).strip() for token_id in top_k_ids[0]]
    return predictions[0] if top_k == 1 else predictions

test_prompts = [
    "India is a",
    "The capital of France is",
    "Machine learning helps us",
    "The quick brown fox",
    "Climate change is a",
    "In 2020, the world",
    "The president of the",
    "Python is a popular",
    "The internet has changed",
    "She opened the door and"
]

print(" Next-Word Predictions:\n")
for prompt in test_prompts:
    next_word = predict_next_word(prompt)
    print(f"{prompt} â†’ {next_word}")

# chatbot
print(" Next Word Bot (type 'exit' to stop)")
while True:
    user_input = input("You: ")
    if user_input.strip().lower() == "exit":
        print("Bot: Goodbye! ")
        break
    next_word = predict_next_word(user_input)
    print(f"Bot: {next_word}")