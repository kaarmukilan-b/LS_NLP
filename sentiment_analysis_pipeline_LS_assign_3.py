# -*- coding: utf-8 -*-
"""Sentiment_analysis_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1514os4yX5alic2uC4b6_RD8LeuWJttev
"""

!pip install -U transformers datasets evaluate scikit-learn
!pip install transformers datasets evaluate scikit-learn
!pip install transformers datasets evaluate

import torch
from datasets import load_dataset
from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer
from transformers import DataCollatorWithPadding
import evaluate
import numpy as np
from transformers import TrainingArguments

# from huggingface_hub import notebook_login
# notebook_login()

# Downgrade fsspec to a stable version that works with Hugging Face
!pip install -U fsspec==2023.6.0

from datasets import load_dataset
dataset = load_dataset("imdb")

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
def tokenize_function(example):
    return tokenizer(example["text"], truncation=True)
#as the dataset tokenizing took long time, and session ranout, i cut it down by 1st tokenising and storing it in drive and then directly using it
#with the below commented command, i created the tokenised dataset and stored it in my drive and after that following is used to acces my drive 
# now i am directly using the tokenised dataset which saves the tokenising runtime of google colab session

#command 
# tokenized_datasets = dataset.map(tokenize_function, batched=True)

# from google.colab import drive
# drive.mount('/content/drive')
# from datasets import load_dataset, DatasetDict
# tokenized_datasets.save_to_disk("/content/drive/MyDrive/imdb_dataset")

#this next line makes me access my drive and read the tokenised dataset directly
from datasets import load_from_disk
tokenized_datasets = load_from_disk("/content/drive/MyDrive/imdb_dataset")

tokenized_datasets = tokenized_datasets.remove_columns(["text"])
tokenized_datasets.set_format("torch")

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

accuracy_metric = evaluate.load("accuracy")
f1_metric = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_metric.compute(predictions=predictions, references=labels)["accuracy"],
        "f1": f1_metric.compute(predictions=predictions, references=labels, average="weighted")["f1"]
    }

import transformers
print(transformers.__version__)
#had to check version due to some error occurred

from transformers import TrainingArguments
print(TrainingArguments.__module__)
import transformers
print(transformers.__version__)

import transformers
print(transformers.__file__)
from transformers import TrainingArguments
print(TrainingArguments.__module__)
print(TrainingArguments.__init__)
from transformers import TrainingArguments
import inspect

print(inspect.signature(TrainingArguments.__init__))

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy= "epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    report_to="none",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"].shuffle(seed=42).select(range(10000)),  # Optional: use a subset
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

"""during this training we can directly see the training at wandb website by creating an API key at https://wandb.ai/authorize and paste it when running the next code or ,

set
report to = "none" in training_arg so it wont be needed to proceed with wandb
another method is importing os and running this

import os

os.environ["WANDB_DISABLED"] = "true"

"""

trainer.train()

model.save_pretrained("sentiment_model")
tokenizer.save_pretrained("sentiment_model")

# Load for inference
model = BertForSequenceClassification.from_pretrained("sentiment_model")
tokenizer = BertTokenizer.from_pretrained("sentiment_model")

texts = ["Movie was great!", "Worst film ever.","movie was not bad", "this is an ok movie","this is an acceptable movie","this is not an acceptable movie", "i didnt enjoy the movie"]

# Tokenize all at once
inputs = tokenizer(texts, truncation=True, padding=True, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)
    preds = torch.argmax(outputs.logits, dim=-1)

for text, pred in zip(texts, preds):
    sentiment = "Positive" if pred.item() == 1 else "Negative"
    print(f"Text: {text}\nPredicted Sentiment: {sentiment}\n")

text = "This movie was not bad!"
inputs = tokenizer(text, return_tensors="pt", truncation=True)
with torch.no_grad():
    logits = model(**inputs).logits
    predicted_class = torch.argmax(logits).item()
print("Sentiment:", "Positive" if predicted_class == 1 else "Negative")
